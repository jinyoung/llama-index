{"docstore/metadata": {"c55f8804-13db-4c25-a7e2-c395bf5d2e5d": {"doc_hash": "c5095589fd3033ca3ebe1260933ebc934bd0c28825b1891a7208fb98bd06280a"}, "6a5c8be2-8eec-4519-92f8-c3634df2b847": {"doc_hash": "166397776c946bcb848b68996dc2dbd5dceb58eb0934493f784d84909dfefb78", "ref_doc_id": "c55f8804-13db-4c25-a7e2-c395bf5d2e5d"}}, "docstore/data": {"6a5c8be2-8eec-4519-92f8-c3634df2b847": {"__data__": {"id_": "6a5c8be2-8eec-4519-92f8-c3634df2b847", "embedding": null, "metadata": {"file_path": "data/contents.txt", "file_name": "contents.txt", "file_type": "text/plain", "file_size": 3716, "creation_date": "2024-02-13", "last_modified_date": "2024-02-13", "last_accessed_date": "2024-02-13"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c55f8804-13db-4c25-a7e2-c395bf5d2e5d", "node_type": "4", "metadata": {"file_path": "data/contents.txt", "file_name": "contents.txt", "file_type": "text/plain", "file_size": 3716, "creation_date": "2024-02-13", "last_modified_date": "2024-02-13", "last_accessed_date": "2024-02-13"}, "hash": "c5095589fd3033ca3ebe1260933ebc934bd0c28825b1891a7208fb98bd06280a", "class_name": "RelatedNodeInfo"}}, "text": "Starter Tutorial\nTip\n\nMake sure you\u2019ve followed the installation steps first.\n\nThis is our famous \u201c5 lines of code\u201d starter example.\n\nDownload data\nThis example uses the text of Paul Graham\u2019s essay, \u201cWhat I Worked On\u201d. This and many other examples can be found in the examples folder of our repo.\n\nThe easiest way to get it is to download it via this link and save it in a folder called data.\n\nSet your OpenAI API key\nLlamaIndex uses OpenAI\u2019s gpt-3.5-turbo by default. Make sure your API key is available to your code by setting it as an environment variable. In MacOS and Linux, this is the command:\n\nexport OPENAI_API_KEY=XXXXX\nand on windows it is\n\nset OPENAI_API_KEY=XXXXX\nLoad data and build an index\nIn the same folder where you created the data folder, create a file called starter.py file with the following:\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nThis builds an index over the documents in the data folder (which in this case just consists of the essay text, but could contain many documents).\n\nYour directory structure should look like this:\n\n\u251c\u2500\u2500 starter.py\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 paul_graham_essay.txt\nQuery your data\nAdd the following lines to starter.py\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\nThis creates an engine for Q&A over your index and asks a simple question. You should get back a response similar to the following: The author wrote short stories and tried to program on an IBM 1401.\n\nViewing Queries and Events Using Logging\nWant to see what\u2019s happening under the hood? Let\u2019s add some logging. Add these lines to the top of starter.py:\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\nYou can set the level to DEBUG for verbose output, or use level=logging.INFO for less.\n\nStoring your index\nBy default, the data you just loaded is stored in memory as a series of vector embeddings. You can save time (and requests to OpenAI) by saving the embeddings to disk. That can be done with this line:\n\nindex.storage_context.persist()\nBy default, this will save the data to the directory storage, but you can change that by passing a persist_dir parameter.\n\nOf course, you don\u2019t get the benefits of persisting unless you load the data. So let\u2019s modify starter.py to generate and store the index if it doesn\u2019t exist, but load it if it does:\n\nimport os.path\nfrom llama_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    StorageContext,\n    load_index_from_storage,\n)\n\n# check if storage already exists\nPERSIST_DIR = \"./storage\"\nif not os.path.exists(PERSIST_DIR):\n    # load the documents and create the index\n    documents = SimpleDirectoryReader(\"data\").load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    # store it for later\n    index.storage_context.persist(persist_dir=PERSIST_DIR)\nelse:\n    # load the existing index\n    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n    index = load_index_from_storage(storage_context)\n\n# either way we can now query the index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\nNow you can efficiently query to your heart\u2019s content! But this is just the beginning of what you can do with LlamaIndex.\n\nNext Steps\n\nlearn more about the high-level concepts.\n\ntell me how to customize things.\n\ncurious about a specific module? check out the guides on the left \ud83d\udc48", "start_char_idx": 0, "end_char_idx": 3667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"c55f8804-13db-4c25-a7e2-c395bf5d2e5d": {"node_ids": ["6a5c8be2-8eec-4519-92f8-c3634df2b847"], "metadata": {"file_path": "data/contents.txt", "file_name": "contents.txt", "file_type": "text/plain", "file_size": 3716, "creation_date": "2024-02-13", "last_modified_date": "2024-02-13", "last_accessed_date": "2024-02-13"}}}}